{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af03183e",
   "metadata": {},
   "source": [
    "# *Retrieval Augmented Generation* con las conferencias mañaneras usando LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263c33f",
   "metadata": {},
   "source": [
    "## Descargando los datos de mi otro repositorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "temporary_directory = tempfile.mkdtemp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f62dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conferencias_repo_url = \"https://github.com/fferegrino/mananeras.git\"\n",
    "conferencias_repo_dir = Path(temporary_directory, \"mananeras\")\n",
    "\n",
    "subprocess.run([\"git\", \"clone\", \"-q\", \"--single-branch\",\n",
    "                \"--branch\", 'cf-llm-1',\n",
    "                \"--depth\", \"1\", conferencias_repo_url, str(conferencias_repo_dir)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3407b029",
   "metadata": {},
   "source": [
    "## Cargando los documentos\n",
    "\n",
    "Todo sistema *RAG* comienza cargando un conjunto inicial de documentos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470b8a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mananeras\n",
    "\n",
    "conferencias = mananeras.lee_todas(conferencias_repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90df55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(conferencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f46f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conferencias[1].titulo)\n",
    "print(conferencias[1].fecha)\n",
    "print(conferencias[1].participaciones[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e87f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogos_presidente = []\n",
    "\n",
    "for conferencia in conferencias:\n",
    "    dialogos_conferencia = []\n",
    "    for participacion in conferencia.participaciones:\n",
    "        hablante = participacion.hablante.lower()\n",
    "        dialogos_participacion = []\n",
    "        if 'andrés manuel' in hablante or 'andrésmanuel' in hablante:\n",
    "            for dialogo in participacion.dialogos:\n",
    "                dialogos_participacion.append(dialogo)\n",
    "        if len(dialogos_participacion) > 0:\n",
    "            dialogos_conferencia.append(\"\\n\".join(dialogos_participacion))\n",
    "    if len(dialogos_conferencia) > 0:\n",
    "        conferencia = {\n",
    "            \"title\": conferencia.titulo,\n",
    "            \"document\": \"\\n\".join(dialogos_conferencia)\n",
    "        }\n",
    "        dialogos_presidente.append(conferencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d0d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dialogos_presidente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dialogos_presidente[0]['document'][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47403534",
   "metadata": {},
   "source": [
    "## Load documents using a document loader\n",
    "\n",
    "Since our text is already in memory, we need to create a custom `DocumentLoader`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b8c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AsyncIterator, Iterator\n",
    "\n",
    "from langchain_core.document_loaders import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "class DialogosPresidenteLoader(BaseLoader):\n",
    "\n",
    "    def __init__(self, conferencias):\n",
    "        self.conferencias = conferencias\n",
    "\n",
    "    def lazy_load(self) -> Iterator[Document]:\n",
    "\n",
    "        for conferencia in self.conferencias:\n",
    "            dialogos_conferencia = []\n",
    "            for participacion in conferencia.participaciones:\n",
    "                hablante = participacion.hablante.lower()\n",
    "                dialogos_participacion = []\n",
    "                if 'andrés manuel' in hablante or 'andrésmanuel' in hablante:\n",
    "                    for dialogo in participacion.dialogos:\n",
    "                        dialogos_participacion.append(dialogo)\n",
    "                if len(dialogos_participacion) > 0:\n",
    "                    dialogos_conferencia.append(\"\\n\".join(dialogos_participacion))\n",
    "            if len(dialogos_conferencia) > 0:\n",
    "                metadata = {\n",
    "                    \"title\": conferencia.titulo,\n",
    "                    \"date\": conferencia.fecha\n",
    "                }\n",
    "\n",
    "                yield Document(\n",
    "                    page_content=\"\\n\".join(dialogos_conferencia),\n",
    "                    metadata=metadata\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255fab37",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogos_presidente_loader = DialogosPresidenteLoader(conferencias)\n",
    "\n",
    "for document in dialogos_presidente_loader.lazy_load():\n",
    "    print(document.metadata['title'])\n",
    "    print(document.metadata['date'])\n",
    "    print(document.page_content[:500])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e38587",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dialogos_presidente_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1cb2fa",
   "metadata": {},
   "source": [
    "## Divide documento en partes (*chunks*)\n",
    "\n",
    "Si estás trabajando con documentos grandes, es importarte dividirlo en partes, que vamos a llamar *chunks*.\n",
    "\n",
    "Esto cumple dos funciones:\n",
    "\n",
    " * Mejorar la relevancia semántica de nuestros embeddings: un documento muy grande puede cubrir demasiados temas, mientras que uno pequeño puede estar más enfocado en un solo tópico\n",
    " * Facilitar la tarea del modelo de lenuaje generativo – *chunks* más pequeños hacen que la ventana de contexto sea más pequeña\n",
    "\n",
    "El proceso de división tiene varios parámetros: el tamaño del *chunk* y el tamaño de traslape entre *chunks*.\n",
    "\n",
    "Existen diversas técnicas de división de documentos, algunas más complejas que otras, la función que estoy usando debajo es una de las más fáciles pero menos recomendables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f708fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300, chunk_overlap=10, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cbd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_splits))\n",
    "print(all_splits[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5896452f-f350-4daf-bf1d-232e3d22b5ca",
   "metadata": {},
   "source": [
    "## Calculando embeddings & almacenándolos en la BD (Chroma)\n",
    "\n",
    "Para generar los embeddings vamos a utilizar un modelo local, descargado de Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores.utils import filter_complex_metadata\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"paraphrase-multilingual-mpnet-base-v2\")\n",
    "\n",
    "vector_store = \"./vector_store\"\n",
    "\n",
    "if Path(vector_store).exists():\n",
    "    vector_store_loaded = Chroma(persist_directory=vector_store, embedding_function=embedding_model)\n",
    "else:\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=filter_complex_metadata(all_splits),\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=\"./vector_store\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14776716",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = embedding_model.embed_query(\"Hola mundo\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f688b221",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc437f3",
   "metadata": {},
   "source": [
    "## Ejecutando queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c3745",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_loaded = Chroma(persist_directory=\"vector_store\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65445ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store_loaded.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 6})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ead5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "pregunta = \"¿Qué significa ser aspiracionista?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdcf835",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(pregunta)\n",
    "\n",
    "for doc in retrieved_docs:\n",
    "    print(doc.metadata['title'])\n",
    "    # print(doc.metadata['date'])\n",
    "    print(doc.page_content[:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefe8e88",
   "metadata": {},
   "source": [
    "## Usando una LLM para generar respuestas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2772dcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "rag_prompt_template = ChatPromptTemplate.from_template(\"\"\"Eres Andrés Manuel Lopez Obrador, presidente de México.\n",
    "Responde a la pregunta basándote en el contexto de lo dicho por el presidente.\n",
    "El contexto está delimitado por las comillas invertidas.\n",
    "Contesta como si la respuesta la estuviera dando Andrés Manuel Lopez Obrador.\n",
    "\n",
    "```\n",
    "{context}\n",
    "```\n",
    "\n",
    "Pregunta: {question}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba6c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_prompt_template.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea765d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b2f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19081280",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in rag_chain.stream(\"¿Qué significa ser aspiracionista?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3217578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def query_llm(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    completions = client.chat.completions.create(\n",
    "\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return completions.choices[0].message.content\n",
    "\n",
    "print(pregunta)\n",
    "print()\n",
    "prompt = \"\"\"Eres Andrés Manuel Lopez Obrador, presidente de México.\n",
    "Responde a la pregunta como si la respuesta la estuviera dando Andrés Manuel Lopez Obrador.\n",
    "\n",
    "Pregunta: {question}\n",
    "\"\"\"\n",
    "final_prompt = prompt.format(question=pregunta)\n",
    "print(query_llm(final_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7734099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
